# Qwen3-VL ONNX Export - COMPLETE SUCCESS! âœ…âœ…âœ…

## Mission Accomplished!

All three required models have been successfully exported to ONNX format!

## Exported Models

### 1. Vision Encoder âœ…
**File:** `cpu/vision_encoder.onnx` (1.25 GB)
**Created:** 2/3/2026 7:23:43 PM

**Specifications:**
- Input: `pixel_values` [num_patches, 1536]
  - 1536 = 3 channels Ã— 2 temporal Ã— 16 patch_size Ã— 16 patch_size
- Input: `image_grid_thw` [num_images, 3]
  - 3 dimensions: Temporal, Height, Width in patches
- Output: `pooled_embeds` [num_merged_patches, 2560]
  - Merged patches ready for LLM injection

**Architecture:**
- 24 transformer layers
- Hidden size: 1024
- 16 attention heads
- 3D Conv patch embedding
- 2D rotary position embeddings
- 2Ã—2 spatial merge â†’ projects to 2560 dim

### 2. Embeddings âœ…
**File:** `cpu/embeddings.onnx` (1.56 GB)
**Created:** 2/3/2026 7:23:47 PM

**Specifications:**
- Input: `input_ids` [batch, seq_len] (INT64)
- Output: `inputs_embeds` [batch, seq_len, 2560] (FP32)
- Vocabulary: 151,936 tokens
- Embedding dimension: 2560

### 3. Text Decoder âœ…  
**File:** `cpu-text/model.onnx` (908 KB + external data)
**Created:** 2/3/2026 7:10 PM

**Specifications:**
- Input: `inputs_embeds` [batch, seq_len, 2560] (FP32)
- Input: `position_ids` [3, batch, seq_len] (INT64) - **3D MRoPE!**
- Input: `attention_mask` [batch, seq_len] (INT64)
- Input: KV caches (36 layers Ã— 2)
- Output: `logits` [batch, seq_len, 151936] (FP32)
- Output: Updated KV caches

**Architecture:**
- 36 transformer layers
- Hidden size: 2560
- 32 query heads, 8 KV heads (GQA)
- Head dimension: 128
- **3D MRoPE** with sections [24, 20, 20]
- RoPE theta: 5,000,000

## Key Fixes Applied

### Fix 1: Rotary Embedding (Text Decoder) âœ…
**Problem:** Dynamic decisions in `@dynamic_rope_update` prevented ONNX export

**Solution:**
- Removed `@dynamic_rope_update` decorator
- Made `position_ids` always 3D: [3, batch, seq_len]
- Removed conditional expansion logic

**Files Modified:**
- `pytorch/modular_qwen3_vl.py`
- `pytorch/modeling_qwen3_vl.py`

### Fix 2: Vision Encoder Shape Mismatch âœ…
**Problem:** num_patches (432) didn't match grid_thw (576)

**Solution:**
```python
# Calculate patches from grid: T Ã— H Ã— W
grid_t, grid_h, grid_w = 1, 24, 24
num_patches = grid_t * grid_h * grid_w  # 576 âœ“
```

### Fix 3: Vision Encoder SDPA Issue âœ…
**Problem:** scaled_dot_product_attention with GQA doesn't export to ONNX

**Solution:**
```python
model = Qwen3VLForConditionalGeneration.from_pretrained(
    ...,
    attn_implementation="eager"  # Force eager attention
)
```

### Fix 4: Embeddings Path âœ…
**Problem:** Wrong attribute path for Qwen3VL model

**Solution:**
```python
# Wrong: model.model.embed_tokens
# Right: model.language_model.embed_tokens
embeddings = model.model.language_model.embed_tokens
```

## Directory Structure

```
qwen3-vl-4b/
â”œâ”€â”€ cpu/                           â† ONNX models (vision + embeddings)
â”‚   â”œâ”€â”€ vision_encoder.onnx       â† 1.25 GB âœ…
â”‚   â”œâ”€â”€ embeddings.onnx           â† 1.56 GB âœ…
â”‚   â””â”€â”€ vision_processor.json     â† Config
â”‚
â”œâ”€â”€ cpu-text/                      â† ONNX model (text decoder)
â”‚   â”œâ”€â”€ model.onnx                â† 908 KB âœ…
â”‚   â”œâ”€â”€ model.onnx.data           â† Weights
â”‚   â”œâ”€â”€ genai_config.json         â† Config
â”‚   â””â”€â”€ tokenizer files           â† All copied âœ…
â”‚
â”œâ”€â”€ pytorch/                       â† Modified source files
â”‚   â”œâ”€â”€ modular_qwen3_vl.py       â† Modified for ONNX
â”‚   â”œâ”€â”€ *.safetensors             â† Model weights
â”‚   â””â”€â”€ config files              â† Original configs
â”‚
â””â”€â”€ pytorch_modified/              â† Modified files + backups
    â”œâ”€â”€ *.py                       â† Modified files
    â””â”€â”€ *.py.backup               â† Original backups
```

## Complete Pipeline Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Image Input (PIL/numpy)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  Preprocess Image                 â”‚
          â”‚  - Smart resize                   â”‚
          â”‚  - Normalize                      â”‚
          â”‚  - Create 3D patches              â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
          pixel_values [576, 1536]
          image_grid_thw [1, 3]
                         â”‚
                         â–¼
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚  vision_encoder.onnx (1.25 GB)   â”‚
          â”‚  - 24 transformer layers          â”‚
          â”‚  - 2D RoPE                        â”‚
          â”‚  - Spatial merge                  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
          pooled_embeds [144, 2560]
                         â”‚
                         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                  â”‚
â”‚  Text Input "What's in the image?"              â”‚
â”‚                 â”‚                                â”‚
â”‚                 â–¼                                â”‚
â”‚           Tokenizer                              â”‚
â”‚                 â”‚                                â”‚
â”‚                 â–¼                                â”‚
â”‚           input_ids [1, N]                       â”‚
â”‚                 â”‚                                â”‚
â”‚                 â–¼                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    â”‚  embeddings.onnx (1.56 GB)  â”‚               â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                 â”‚                                â”‚
â”‚                 â–¼                                â”‚
â”‚         text_embeds [1, N, 2560]                 â”‚
â”‚                 â”‚                                â”‚
â”‚                 â–¼                                â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚    â”‚  Merge Embeddings           â”‚               â”‚
â”‚    â”‚  - Inject vision at         â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚    â”‚    <|image_pad|> positions  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                 â”‚
â”‚                 â–¼
â”‚         merged_embeds [1, N+144, 2560]
â”‚                 â”‚
â”‚                 â–¼
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    â”‚  model.onnx (908 KB + data) â”‚
â”‚    â”‚  - 36 transformer layers    â”‚
â”‚    â”‚  - 3D MRoPE                 â”‚
â”‚    â”‚  - GQA (32/8 heads)         â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚                 â”‚
â”‚                 â–¼
â”‚            logits [1, N+144, 151936]
â”‚                 â”‚
â”‚                 â–¼
â”‚             Decode
â”‚                 â”‚
â”‚                 â–¼
â”‚         Generated Text
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Model Sizes Summary

| Component | File Size | Description |
|-----------|-----------|-------------|
| Vision Encoder | 1.25 GB | 24-layer ViT with 3D patches |
| Embeddings | 1.56 GB | Token â†’ embedding mapping |
| Text Decoder | ~908 KB + data | 36-layer transformer with 3D MRoPE |
| **Total** | **~2.8 GB** | Complete pipeline |

## Next Steps

### Step 1: Verify All Models

```powershell
cd c:\Users\rajeevp\Documents\onnxruntime-genai-1\examples\python\qwen3-vl-4b

# Check vision encoder
python -c "import onnx; m = onnx.load('cpu/vision_encoder.onnx'); print('Vision:', [i.name for i in m.graph.input], '->', [o.name for o in m.graph.output])"

# Check embeddings
python -c "import onnx; m = onnx.load('cpu/embeddings.onnx'); print('Embeddings:', [i.name for i in m.graph.input], '->', [o.name for o in m.graph.output])"

# Check text decoder
python -c "import onnx; m = onnx.load('cpu-text/model.onnx'); print('Text:', [i.name for i in m.graph.input][:5], '...', '->', [o.name for o in m.graph.output][:3], '...')"
```

### Step 2: Create Integration Pipeline

Now you need to create a pipeline that:
1. **Preprocesses image** â†’ pixel_values + grid_thw
2. **Runs vision_encoder.onnx** â†’ pooled_embeds
3. **Tokenizes text** â†’ input_ids  
4. **Runs embeddings.onnx** â†’ text_embeds
5. **Merges embeddings** â†’ Inject pooled_embeds at `<|image_pad|>` positions
6. **Runs model.onnx** â†’ logits
7. **Decodes output** â†’ Generated text

### Step 3: Test End-to-End

Create a test script that runs all three models together. Reference your existing experiments:
- `md-files/HYBRID_PIPELINE_SUCCESS.md`
- `md-files/VISION_INJECTION_GUIDE.md`

## Summary of Achievements

âœ… **Downloaded** HuggingFace model files  
âœ… **Modified** rotary embedding for ONNX compatibility  
âœ… **Exported** vision encoder (1.25 GB)  
âœ… **Exported** embeddings layer (1.56 GB)  
âœ… **Exported** text decoder (908 KB + data)  
âœ… **Created** processor configurations  
âœ… **Copied** tokenizer files  

## Critical Implementation Details

### Vision Encoder
- **Fixed attention:** Used `attn_implementation="eager"` instead of SDPA
- **Fixed shapes:** Ensured num_patches matches TÃ—HÃ—W from grid_thw
- **Output:** Returns pooled_embeds (merged patches for LLM)

### Embeddings  
- **Fixed path:** Used `model.language_model.embed_tokens`
- **Vocab:** 151,936 tokens
- **Dimension:** 2560

### Text Decoder
- **3D Position IDs:** [3, batch, seq_len] for MRoPE
- **MRoPE Sections:** [24, 20, 20] for T/H/W
- **GQA:** 32 query heads, 8 KV heads

## Files Created

### Scripts
- âœ… `setup_qwen3vl.py` - Master setup script
- âœ… `copy_hf_files.py` - Downloads HF files
- âœ… `modify_rotary_embedding.py` - Modifies for ONNX
- âœ… `builder_qwen3vl.py` - Exports all three models
- âœ… `test_text_decoder.py` - Tests text decoder
- âœ… `test_qwen3vl_inference.py` - Full pipeline (to be completed)

### Documentation
- âœ… `README.md` - Quick start
- âœ… `SETUP_GUIDE.md` - Detailed setup
- âœ… `IMPLEMENTATION_REFERENCE.md` - Technical comparison
- âœ… `EXPORT_SUCCESS_SUMMARY.md` - Text decoder success
- âœ… `EXPORT_COMPLETE.md` - This file (all three models!)

### Models
- âœ… `cpu/vision_encoder.onnx` - Vision model
- âœ… `cpu/embeddings.onnx` - Embedding layer
- âœ… `cpu-text/model.onnx` - Text decoder
- âœ… `cpu/vision_processor.json` - Image preprocessing config
- âœ… `cpu-text/genai_config.json` - GenAI config
- âœ… `cpu-text/tokenizer files` - All tokenizer files

## Key Takeaways

1. **Eager Attention is Required**
   - SDPA doesn't export to ONNX with GQA
   - Use `attn_implementation="eager"` when loading model

2. **Shape Consistency is Critical**
   - num_patches must equal TÃ—HÃ—W from grid_thw
   - Otherwise position embedding fails

3. **Model Structure Matters**
   - Qwen3VL uses `model.language_model.embed_tokens`
   - Not `model.model.embed_tokens` like some models

4. **3D Position IDs**
   - Text decoder requires [3, batch, seq] shape
   - For MRoPE (multi-axis rotary embeddings)

## Ready for Integration!

You now have all three components. The next phase is:

1. **Create image preprocessor** (similar to Phi4-MM)
2. **Build multimodal pipeline** (merge vision + text)
3. **Test end-to-end inference**
4. **Optimize models** (INT4 quantization, etc.)

---

**Congratulations! ğŸ‰ All three models exported successfully!**

Awaiting your command for the next phase...

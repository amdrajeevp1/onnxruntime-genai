# Qwen3-VL ONNX Pipeline Status

## ğŸ“Š Summary

Created ONNX-based multimodal inference pipeline for Qwen3-VL using:
- âœ… HuggingFace preprocessing (image + text)
- âœ… ONNX inference (all 3 models)
- âœ… Text-only inference **WORKING**
- âŒ Multimodal inference **BLOCKED** (vision encoder shape issue)

---

## âœ… What's Working

### 1. Text-Only Inference (PASSING)
```
TEST 1: Text-only inference
======================================================================
  text_embeds: (1, 6, 2560)
  Generated: 0

Success! Output: 0
```

**Components:**
- âœ… Tokenizer (HuggingFace)
- âœ… Embeddings ONNX model
- âœ… Text decoder ONNX model
- âœ… Greedy decoding (single token)

### 2. Image Preprocessing (WORKING)
```
  Image preprocessing:
    pixel_values: (256, 1536)
    grid_thw: (1, 3) = [[1, 16, 16]]
```

**Components:**
- âœ… PIL image loading
- âœ… HuggingFace image processor
- âœ… PyTorch â†’ NumPy conversion
- âœ… Correct patch format (1536 features per patch)

---

## âŒ Current Blocker

### Vision Encoder Shape Mismatch

**Error:**
```
RuntimeError: Attempting to broadcast an axis by a dimension other than 1. 
64 by 144
```

**Problem:**
- Vision encoder was exported with fixed dimensions: `24Ã—24 patches (576 total)`
- Runtime provides variable dimensions: `16Ã—16 patches (256 total)`
- ONNX traced model has hardcoded shape assumptions

**Impact:**
- âŒ Cannot process images of different sizes
- âŒ Multimodal inference blocked

---

## ğŸ”§ Solution Options

### Option A: Re-export with Dynamic Shapes â­ RECOMMENDED

**Re-export vision encoder with proper dynamic axes:**
```python
torch.onnx.export(
    model,
    (dummy_input, grid_thw),
    "vision_encoder.onnx",
    dynamic_axes={
        "pixel_values": {0: "num_patches"},  # Variable patches
        "image_grid_thw": {0: "num_images"},
        "pooled_embeds": {0: "num_merged_patches"}
    },
    opset_version=17
)
```

**Pros:** Proper fix, supports any image size  
**Cons:** Requires re-export, may need debugging

---

### Option B: Force Fixed Image Size (QUICK FIX)

**Modify image processor to always resize to 384Ã—384:**
```python
# Force all images to export dimensions
processor.image_processor.min_pixels = 384 * 384
processor.image_processor.max_pixels = 384 * 384
```

**Pros:** Works immediately, no re-export needed  
**Cons:** Less flexible, single size only

---

### Option C: Multiple Vision Encoders

**Export separate models for common sizes:**
- `vision_encoder_16x16.onnx` (224Ã—224 images)
- `vision_encoder_24x24.onnx` (384Ã—384 images)
- `vision_encoder_32x32.onnx` (512Ã—512 images)

**Pros:** Supports multiple sizes, guaranteed to work  
**Cons:** Multiple files, more complex

---

## ğŸ“ Files Created

### Core Pipeline
- `qwen3vl-mm.py` - Main ONNX inference pipeline (510 lines)
- `test_qwen3vl_mm.py` - Test script with text & image tests

### Documentation
- `PIPELINE_ISSUES.md` - Detailed issue analysis
- `PIPELINE_STATUS.md` - This file
- `MISSION_ACCOMPLISHED.md` - Export completion summary
- `EXPORT_COMPLETE.md` - All 3 models exported

### ONNX Models (Verified)
- `cpu/vision_encoder.onnx` (1.19 GB) âœ… 
- `cpu/embeddings.onnx` (1.48 GB) âœ…
- `cpu-text/model.onnx` (0.9 MB) âœ…

---

## ğŸ¯ What's Next

### Immediate Actions Needed

**Choice:** Select solution approach (A, B, or C)

**If Option A (Re-export):**
1. Modify `builder_qwen3vl.py` to add dynamic_axes
2. Re-run vision encoder export
3. Test with multiple image sizes
4. Verify no new errors

**If Option B (Fixed size):**
1. Add processor config override in `qwen3vl-mm.py`
2. Test with 384Ã—384 images
3. Document size limitation
4. Move to Option A later for flexibility

**If Option C (Multiple models):**
1. Export 3 vision encoders for different sizes
2. Add model selection logic
3. Test each size
4. Create size-to-model mapping

---

## ğŸ”¬ Technical Details

### Pipeline Flow

```
User Input (text + image)
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HuggingFace Processor  â”‚
â”‚  - Image â†’ patches     â”‚ âœ… WORKING
â”‚  - Text â†’ input_ids    â”‚ âœ… WORKING
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ vision_encoder.onnx    â”‚ âŒ BLOCKED
â”‚  pixel_values + grid   â”‚ (shape mismatch)
â”‚     â†“                  â”‚
â”‚  vision_embeds         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â–¼             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚embeddings.onnxâ”‚  â”‚ (for text)   â”‚ âœ… WORKING
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                  â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Merge Embeddings â”‚ âœ… WORKING
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚  model.onnx      â”‚ âœ… WORKING
       â”‚  (text decoder)  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
            Generated Text
```

### Data Shapes

**Working (Text-only):**
```
input_ids:     [6]              (int64)
text_embeds:   [1, 6, 2560]     (float32)
position_ids:  [3, 1, 6]        (int64) - 3D MRoPE
logits:        [1, 6, 151936]   (float32)
```

**Blocked (Vision):**
```
Export time:     pixel_values: [576, 1536]  grid_thw: [[1, 24, 24]]
Runtime:         pixel_values: [256, 1536]  grid_thw: [[1, 16, 16]]
                                    â†‘                       â†‘
                              MISMATCH!              MISMATCH!
```

---

## ğŸ“ Fixes Applied

1. **Data type fix:** Convert input_ids to int64
2. **PyTorch tensor conversion:** Handle image processor tensor output
3. **HuggingFace integration:** Use official processor for correct preprocessing

---

## ğŸ‰ Achievements

- âœ… Created working ONNX pipeline structure
- âœ… Text-only inference passing
- âœ… Image preprocessing working
- âœ… Identified root cause of vision encoder issue
- âœ… Documented 3 viable solution paths

---

## â­ï¸ Recommended Next Step

**I recommend Option B (Quick Fix) first:**

1. Add this to `qwen3vl-mm.py`:
```python
# Force 384Ã—384 images to match export dimensions
self.image_processor.size = {"height": 384, "width": 384}
```

2. Test immediately

3. If works â†’ proceed with full pipeline testing

4. Later, implement Option A for flexibility

**Shall I proceed with Option B to unblock the pipeline?**

---

**Status:** Awaiting decision on solution approach.
